
<openai-chat-container #chatbot 
      
[persistChat]="false"
[chatAlignment]="'center'"
[chatIdentifier]="chatComponentIdentifier"
[chatContextLength]="6"
[chatQuerySamplesFileUri]="chatQuerySamplesFileURIPath"
[showFeedbackOptions]="true"
[showContentDisclaimer]="true"
[contentDisclaimerMessage]="contentDisclaimerMessage"
[quotaEnforced]="true"
[dailyMessageQuota]="20"
[messageQuotaWarningThreshold]="10"
[chatHeader]= "chatHeader"
[systemPhotoSource]="'/assets/img/openailogo.svg'"
[customFirstMessage] = "customFirstMessageEdit"
[apiProtocol] = "apiProtocol"
[inputTextLimit] = "inputTextLimit"
[chatModel] = "chatModel"
[showCopyOption] = "showCopyOption"
[onCopyClick] = "copyChatGPTClicked"
>
</openai-chat-container>
<!-- 
<html><h1>5 Whys analysis</h1> <h2>Why did we not identify the regression earlier?</h2><p><b>Miss in Manual Testing before Code check in:</b></p> <p>Code change was to remove RDFE management cert from file servers. This was done as a security precaution to removed unused certs from file server and was primarily needed for ASEv2.</p> <p>On Cloud service Megastamps and VMSS stamps this cert is unused, and we validated the change manually on Cloud Service, VMSS and ASEv2 before the change was integrated to master.</p> <p>Regression was caused as the RDFE cert was used in flex stamps data tenant for public ip discovery.</p> <p><b>Deployment SDP miss:</b></p> <p>Change was checked into master branch on 03/29/2023 and the regression happened on 04/06/23. During this time, the change was never deployed to data tenant of the flex stamp.</p><p>All the flex stamps in EUAP have been or in the process of being converted to VMSS.</p> <p>Build from master branch were deployed to EUAP stamps before they were rolled out to stage 3. Last deployment to the data tenant on euapdm1-505 was on 2023-03-28 20:34 (Build version: 100.0.7.161) and to msftinthk1 on 2023-03-26 02:30 (Build Version)</p> <p>NOTE: We did deploy to EUAP on 2023-03-31 08:33 (Build version: 100.0.7.171) but this was only to the main tenant and not the flex tenant</p> <p>Flex status = </p> <ul> <li>There are 3 flex stamps in stage 0 and 1</li> <li>Around 40 flex stamps in Stage 2 but they are all on an older build (Build version: 100.0.7.116)</li> </ul> <h2>What was the impact of the regression on File Servers?</h2> <p>On flex stamps, we have 12 file servers on data tenant and 25 storage volumes. In that case, the split would be 8 Read-Write file servers and 4 Read-Only</p>  <p><b>Each UD rolled out the regression to a batch of File Servers and we ran out of healthy File Servers to mount volumes (Next question addresses why we didn't identify that file servers got into a bad state after each UD):</b></p> <p>First UD, upgrades 1 file servers and subsequent UDs upgrade 2 file servers per UD.</p><p>After the 3rd UD, when 5 file servers are upgraded, we run out of capacity to mount all the storage volumes.</p>  <p>In hindsight, if we were able to identify that the file servers got into a bad state after each UD, we could have prevented the issue.</p> <p>Till the 3rd UD, we had enough capacity to mount all storage volumes and we didn't identify that the file servers got into a bad state after each UD</p> <p>Once the Read Write volumes become unavailable, the file servers fall back to using Read Only volumes. But as the UD continue, we end up upgrading more file servers and soon we run out of file servers to mount all the Read Only volumes.</p><h2>Why did the deployments not pause because of the regression?</h2><p><b>Exceptions in role's Run( ) are unmonitored</b></p><p>Code to retrieve the RDFE cert was in the Run( ) method of HostingFileServerRole and not in the OnStart( ). When we hit the exception in retrieving RDFE cert, this ended up as an unhandled exception and results in failing to initialize Storage Controller library.</p> <p>Run( ) method holds the runtime logic of each of our roles and Exceptions in Run( ) are not being monitored. This resulted in any file server being upgraded getting into a bad state and being stuck in a Run() loop, and continuously hitting an exception for the missing cert and failing to initialize correctly.</p> <p>We probably need to see if we need to block deployments when there is an exception in the Run( ) logic, as this pattern of issues can occur again resulting in deployments not being paused when we fail to initialize correctly. </p> <p>Same pattern seems to be prevalent on other roles too: <a href="https://msazure.visualstudio.com/One/_git/AAPT-Antares-Websites?path=/src/Hosting/Microsoft.Web.Hosting.Runtime.Roles/Data/HostingDataRole.cs&version=GBdev&_a=contents" target="_blank">HostingDataRole.cs - Repos (visualstudio.com)</a>, <a href="https://msazure.visualstudio.com/One/_git/AAPT-Antares-Websites?path=/src/Hosting/Microsoft.Web.Hosting.Runtime.Roles/FrontEnd/HostingFrontEndRole.cs&version=GBdev&_a=contents" target="_blank">HostingFrontEndRole.cs - Repos (visualstudio.com).</a></p> <p>We have a big..try catch around the Run( ) method and have alerting to monitor if the Run( ) method throws an exception.</p> <h2>Why were a lot of customers not notified at the time of the incident?</h2><p>Canaries use Dynamic Cache and continued to report Read Success even though there is no ReadWrite or Read Only Volumes mounted. So the canaries didn't pickup failures even though both ReadWrite and ReadOnly are missing.</p><p>Blu-023 had 5 volumes with both RW and RO missing at 10:00 but Canary still had not fired</p><p><b>Number of Volumes missing both RW and RO</b></p><p><b>Read Success to sites on Volume-19 even when there are no RW and RO volumes mounted (Dynamic Cache)</b></p><h2>How many sites were impacted?</h2><p>Exact site impact is hard to measure as the subset of impacted sites were the ones for who both the primary and standby storage volumes are not available.</p><p>At the time of the incident, we assessed the impact by identifying all the sites that had were using storage volume and had 500 level errors</p><p>But this graph below shows the increasing from around 680 sites that were returning 500 errors before the incident to around 2500 sites that throw 500 error during the peak of the issue. So the impact is most likely around 1800 sites at the peak of the issue</p><h1>Detection and mitigation</h1><h2>Detection Source</h2><p>Monitoring</p><h2>Detection Details</h2><p>LSI Sev 1 Alert</p><h2>Mitigation Steps</h2><p>Monitoring identified the unhealthy change and automatically stopped the deployment to help prevent additional scale units from being impacted. To mitigate, we developed and applied a config update mitigation that corrected the code configuration issue, which was rolled out to the affected scale units that received the unhealthy deployment.</p><h2>Fix</h2><p>Fixed with Ad-Hoc steps</p><h1>Contributing Factors</h1><h2>Root Cause Title</h2><p>[Public] [AppServiceDP][AutoComms], Canary Sites Two Percent Failure - waws-prod-blu-023</p><h2>Root Cause Details</h2><p>The Microsoft Azure Team has investigated the issue reported regarding the HTTP 500 level errors that your app experienced.</p> <p>On 04/06/2023, App Service rolled out a configuration change to our scale units in East US and North Central US. The config change was part of our platform upgrade and was performed to enhanced reliability and security on our scale units.<br>Unfortunately on a subset of our scale units, this change impacted the ability of the front ends to access the storage subsystem. As a result, your app might have experienced read/write access failures to files.Unfortunately on a subset of our scale units, this change impacted the ability of the front ends to access the storage subsystem. As a result, your app might have experienced read/write access failures to files.</p> <p>The issue was automatically detected and the upgrade was immediately paused. To mitigate the issue, engineers  reverted the config change on all the impacted scale units. Additionally, we have have setup verification to ensure that all the impacted apps are mitigated.</p> <p>We are continuously taking steps to improve the Azure Web App service and our processes to ensure such incidents do not occur in the future, and in this case that includes (but is not limited to):</p> <ul> <b><li>Enhanced monitoring and notification of instability in the storage subsystem</li></b> <b><li>Enhanced testing to ensure any potential issues with config change roll our are identified early</li></b> </ul> <p>We apologize for any inconvenience.</p> <p>Regards,<br>The Microsoft Azure Team<br><a href="https://privacy.microsoft.com/en-us/privacystatement" target="_blank">Privacy Statement</a></p><h2>Root Cause Category</h2><p>Service-Configuration</p><h2>Root Cause Subcategory</h2><p>Config change on File Servers</p><h2>Caused by Change?</h2><p>Yes</p>\n\n Always give your responses in html format. Internal and External PIRs are written differently. When asked for an internal PIR example, do not follow the formatting for an external PIR, public PIR, or private PIR."></html> -->